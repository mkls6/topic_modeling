{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "env = getenv('DATASET_DIR')\n",
    "\n",
    "NEWSGROUP_HOME = env if env is not None else '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, LdaModel, LdaMulticore\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', data_home=NEWSGROUP_HOME)\n",
    "test = fetch_20newsgroups(subset='test', data_home=NEWSGROUP_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dchhabra@stpl.ists.ca (Deepak Chhabra)\n",
      "Subject: Re: Goalie masks\n",
      "Nntp-Posting-Host: stpl.ists.ca\n",
      "Organization: Solar Terresterial Physics Laboratory, ISTS\n",
      "Lines: 21\n",
      "\n",
      "In article <120666@netnews.upenn.edu> kkeller@mail.sas.upenn.edu (Keith Keller) writes:\n",
      ">My vote goes to John Vanbiesbrouck.  His mask has a skyline of New York\n",
      ">City, and on the sides there are a bunch of bees (Beezer).  It looks\n",
      ">really sharp.\n",
      "\n",
      "Funny you should mention this; one time on HNIC Don Cherry pointed out\n",
      "Vanbiesbrouck's mask.  He _hated_ it.  I think he said something to the effect\n",
      "of:\n",
      "\"You see?  He was great last year; now he goes out and gets that dopey mask \n",
      "and he can't stop a beachball!\"\n",
      "\n",
      "You may or may not take Cherry seriously at all, but I cracked up when I heard\n",
      "it.\n",
      "\n",
      "I think Ed Belfour has the current best mask in the NHL btw.  I also like\n",
      "Moog's, and I'll give Fuhr's new one an honourable mention, although I haven't\n",
      "seen it closely yet (it looked good from a distance!).  What's also neat is\n",
      "Chevaldae's in Detroit; they call him \"Chevy\" so he has two checkered flags\n",
      "painted at the top as in an auto race.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][35], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "stop_list = [\n",
    "    \"subject\",\n",
    "    \"from\",\n",
    "    \"/\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \":\",\n",
    "    \"re\",\n",
    "    \"nntp\",\n",
    "    \"posting\",\n",
    "    \"host\",\n",
    "    \"lines\",\n",
    "    \"write\",\n",
    "    \"organization\",\n",
    "    \"keyword\",\n",
    "    \"distribution\",\n",
    "    \"news\",\n",
    "    \"software\",\n",
    "    \"university\",\n",
    "    \"like\",\n",
    "    \"think\",\n",
    "    \"+\",\n",
    "    \"$\",\n",
    "    \"s\",\n",
    "    \">\",\n",
    "    \"<\",\n",
    "    \"C\",\n",
    "    \"year\",\n",
    "    \"|\",\n",
    "    \"=\",\n",
    "    \"nt\",\n",
    "    \"o\",\n",
    "    \"article\",\n",
    "    \n",
    "]\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out emails and other irrelevant stuff from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    doc = [token.lemma_ for token in doc\n",
    "           if not (token.is_stop or\n",
    "                   token.is_punct or\n",
    "                   token.like_email or\n",
    "                   token.like_url or\n",
    "                   token.is_space or\n",
    "                   token.like_num or\n",
    "                   token.lemma_.lower() in stop_list)]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*train['data'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process texts, make a dictionary and a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# doc_lst = []\n",
    "\n",
    "for i, sent in enumerate(train['data']):\n",
    "    sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "    sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "    sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "    # sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "    # doc_lst.append(sent)\n",
    "    train['data'][i] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = list(nlp.pipe(train['data']))  # Limit number of entries for quicker analysis\n",
    "dictionary = Dictionary(doc_lst)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thing', 'car', 'Maryland', 'College', 'Park', 'wonder', 'enlighten', 'car', 'see', 'day', '2-door', 'sport', 'car', 'look', 'late', '60s/', 'early', '70s', 'call', 'Bricklin', 'door', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'spec', 'production', 'car', 'history', 'info', 'funky', 'look', 'car', 'e', 'mail', 'Thanks', 'IL', 'bring', 'neighborhood', 'Lerxst']\n",
      "['Guy', 'Kuo', 'SI', 'Clock', 'Poll', 'Final', 'Summary', 'Final', 'SI', 'clock', 'report', 'Keywords', 'SI', 'acceleration', 'clock', 'upgrade', 'I.D.', 'shelley.1qvfo9INNc3s', 'Washington', 'fair', 'numb', 'brave', 'soul', 'upgrade', 'SI', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', 'speed', 'attain', 'CPU', 'rate', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', 'be', 'floppy', 'especially', 'request', 'summarize', 'day', 'add', 'network', 'knowledge', 'base', 'clock', 'upgrade', 'answer', 'poll', 'Thanks', 'Guy', 'Kuo']\n",
      "['Thomas', 'E', 'Willis', 'PB', 'question', 'Purdue', 'Engineering', 'Computer', 'Network', 'usa', 'folk', 'mac', 'plus', 'finally', 'give', 'ghost', 'weekend', 'start', 'life', '512k', 'way', 'sooo', 'be', 'market', 'new', 'machine', 'bite', 'soon', 'intend', 'be', 'look', 'pick', 'powerbook', 'maybe', 'bunch', 'question', 'hopefully', 'somebody', 'answer', 'anybody', 'know', 'dirt', 'round', 'powerbook', 'introduction', 'expect', 'have', 'hear', '185c', 'suppose', 'appearence', 'summer', 'hear', 'anymore', 'access', 'macleak', 'wonder', 'anybody', 'info', 'anybody', 'hear', 'rumor', 'price', 'drop', 'powerbook', 'line', 'one', 'duo', 'go', 'recently', 'impression', 'display', 'probably', 'swing', 'get', '80Mb', 'disk', 'feel', 'well', 'display', 'yea', 'look', 'great', 'store', 'wow', 'good', 'solicit', 'opinion', 'people', 'use', 'day', 'day', 'worth', 'take', 'disk', 'size', 'money', 'hit', 'active', 'display', 'realize', 'real', 'subjective', 'question', 'have', 'play', 'machine', 'computer', 'store', 'breifly', 'figure', 'opinion', 'somebody', 'actually', 'use', 'machine', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'thank', 'bunch', 'advance', 'info', 'email', 'ill', 'post', 'summary', 'read', 'time', 'premium', 'final', 'corner', 'Tom', 'Willis', 'Purdue', 'Electrical', 'Engineering', 'Convictions', 'dangerous', 'enemy', 'truth', 'lie', 'F.', 'W.', 'Nietzsche']\n"
     ]
    }
   ],
   "source": [
    "print(*doc_lst[:3], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda_model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "#                      num_topics=10, random_state=2,\n",
    "#                      update_every=, passes=20,\n",
    "#                      alpha='auto', per_word_topics=True)\n",
    "lda_model = LdaMulticore(workers=6, corpus=corpus, id2word=dictionary,\n",
    "                         num_topics=20, random_state=2,\n",
    "                         per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.005*\"people\" + 0.004*\"X\" + 0.004*\"know\" + 0.003*\"right\" + 0.003*\"time\" + 0.003*\"have\" + 0.003*\"way\" + 0.003*\"say\" + 0.003*\"go\" + 0.003*\"thing\"'),\n",
       " (1,\n",
       "  '0.006*\"have\" + 0.004*\"know\" + 0.003*\"be\" + 0.003*\"well\" + 0.003*\"use\" + 0.003*\"people\" + 0.003*\"time\" + 0.003*\"try\" + 0.003*\"get\" + 0.003*\"say\"'),\n",
       " (2,\n",
       "  '0.006*\"X\" + 0.005*\"have\" + 0.003*\"work\" + 0.003*\"time\" + 0.002*\"people\" + 0.002*\"use\" + 0.002*\"be\" + 0.002*\"right\" + 0.002*\"look\" + 0.002*\"know\"'),\n",
       " (3,\n",
       "  '0.004*\"drive\" + 0.003*\"know\" + 0.003*\"people\" + 0.003*\"thing\" + 0.003*\"way\" + 0.003*\"have\" + 0.003*\"use\" + 0.003*\"need\" + 0.003*\"look\" + 0.003*\"time\"'),\n",
       " (4,\n",
       "  '0.004*\"say\" + 0.003*\"people\" + 0.003*\"right\" + 0.002*\"have\" + 0.002*\"key\" + 0.002*\"use\" + 0.002*\"be\" + 0.002*\"X\" + 0.002*\"time\" + 0.002*\"thing\"'),\n",
       " (5,\n",
       "  '0.005*\"know\" + 0.004*\"time\" + 0.004*\"say\" + 0.004*\"have\" + 0.003*\"need\" + 0.003*\"go\" + 0.003*\"people\" + 0.003*\"use\" + 0.003*\"be\" + 0.003*\"good\"'),\n",
       " (6,\n",
       "  '0.005*\"X\" + 0.005*\"have\" + 0.004*\"know\" + 0.004*\"be\" + 0.004*\"good\" + 0.003*\"say\" + 0.003*\"game\" + 0.002*\"get\" + 0.002*\"want\" + 0.002*\"go\"'),\n",
       " (7,\n",
       "  '0.013*\"X\" + 0.006*\"have\" + 0.005*\"people\" + 0.004*\"know\" + 0.003*\"use\" + 0.003*\"need\" + 0.003*\"new\" + 0.003*\"want\" + 0.003*\"file\" + 0.003*\"be\"'),\n",
       " (8,\n",
       "  '0.008*\"X\" + 0.008*\"AX\" + 0.005*\"know\" + 0.004*\"problem\" + 0.003*\"people\" + 0.003*\"want\" + 0.002*\"time\" + 0.002*\"say\" + 0.002*\"well\" + 0.002*\"be\"'),\n",
       " (9,\n",
       "  '0.005*\"know\" + 0.005*\"time\" + 0.004*\"problem\" + 0.003*\"want\" + 0.003*\"system\" + 0.003*\"work\" + 0.003*\"use\" + 0.003*\"have\" + 0.002*\"people\" + 0.002*\"X\"'),\n",
       " (10,\n",
       "  '0.006*\"`\" + 0.004*\"know\" + 0.004*\"X\" + 0.003*\"people\" + 0.003*\"time\" + 0.003*\"say\" + 0.003*\"use\" + 0.003*\"have\" + 0.003*\"key\" + 0.003*\"God\"'),\n",
       " (11,\n",
       "  '0.005*\"know\" + 0.004*\"system\" + 0.004*\"use\" + 0.003*\"time\" + 0.003*\"well\" + 0.003*\"have\" + 0.003*\"want\" + 0.003*\"new\" + 0.003*\"people\" + 0.002*\"way\"'),\n",
       " (12,\n",
       "  '0.010*\"X\" + 0.004*\"know\" + 0.003*\"people\" + 0.003*\"have\" + 0.002*\"right\" + 0.002*\"find\" + 0.002*\"say\" + 0.002*\"`\" + 0.002*\"come\" + 0.002*\"use\"'),\n",
       " (13,\n",
       "  '0.005*\"people\" + 0.005*\"say\" + 0.004*\"time\" + 0.004*\"know\" + 0.004*\"come\" + 0.004*\"go\" + 0.003*\"have\" + 0.003*\"use\" + 0.003*\"right\" + 0.003*\"drive\"'),\n",
       " (14,\n",
       "  '0.007*\"know\" + 0.005*\"people\" + 0.004*\"have\" + 0.003*\"time\" + 0.003*\"be\" + 0.003*\"go\" + 0.003*\"work\" + 0.003*\"want\" + 0.002*\"get\" + 0.002*\"say\"'),\n",
       " (15,\n",
       "  '0.004*\"work\" + 0.004*\"use\" + 0.004*\"run\" + 0.004*\"problem\" + 0.003*\"have\" + 0.003*\"Windows\" + 0.003*\"X\" + 0.002*\"be\" + 0.002*\"know\" + 0.002*\"try\"'),\n",
       " (16,\n",
       "  '0.010*\"X\" + 0.005*\"know\" + 0.004*\"people\" + 0.004*\"use\" + 0.003*\"have\" + 0.003*\"time\" + 0.003*\"file\" + 0.003*\"look\" + 0.003*\"say\" + 0.002*\"window\"'),\n",
       " (17,\n",
       "  '0.005*\"know\" + 0.005*\"file\" + 0.004*\"DB\" + 0.004*\"have\" + 0.004*\"need\" + 0.004*\"problem\" + 0.004*\"be\" + 0.003*\"find\" + 0.003*\"time\" + 0.003*\"work\"'),\n",
       " (18,\n",
       "  '0.003*\"people\" + 0.003*\"X\" + 0.003*\"work\" + 0.002*\"time\" + 0.002*\"go\" + 0.002*\"good\" + 0.002*\"be\" + 0.002*\"know\" + 0.002*\"thing\" + 0.002*\"use\"'),\n",
       " (19,\n",
       "  '0.444*\"AX\" + 0.031*\"MAX\" + 0.007*\"G9V\" + 0.006*\"G)R\" + 0.004*\"people\" + 0.002*\"say\" + 0.002*\"M\" + 0.001*\"GIZ\" + 0.001*\"know\" + 0.001*\"good\"')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=train['data']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "# df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=train['data'])\n",
    "\n",
    "# # Format\n",
    "# df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "# df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Something here constantly kills the kernelâ€¦\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_list,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud, interpolation='linear')\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence metric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_lst, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "coherence_lda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
