{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "env = getenv('DATASET_DIR')\n",
    "\n",
    "NEWSGROUP_HOME = env if env is not None else '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', data_home=NEWSGROUP_HOME)\n",
    "test = fetch_20newsgroups(subset='test', data_home=NEWSGROUP_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_list = [\n",
    "    \"subject\",\n",
    "    \"from\",\n",
    "    \"/\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \":\",\n",
    "    \"nntp\",\n",
    "    \"posting\",\n",
    "    \"host\",\n",
    "    \"lines\",\n",
    "    \"organization\",\n",
    "    \"keywords\",\n",
    "    \"distribution\",\n",
    "    \"news\",\n",
    "    \"software\",\n",
    "    \"university\",\n",
    "    \"$\",\n",
    "    \"s\",\n",
    "    \">\",\n",
    "    \"|\",\n",
    "    \"=\",\n",
    "    \"nt\",\n",
    "    \"o\",\n",
    "    \"article\",\n",
    "    \n",
    "]\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out emails and other irrelevant stuff from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    doc = [token.lemma_ for token in doc\n",
    "           if not (token.is_stop or\n",
    "                   token.is_punct or\n",
    "                   token.like_email or\n",
    "                   token.like_url or\n",
    "                   token.is_space or\n",
    "                   token.like_num or\n",
    "                   token.lemma_.lower() in stop_list)]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*train['data'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process texts, make a dictionary and a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i, sent in enumerate(train['data']):\n",
    "    sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "    sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "    sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "    train['data'][i] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = list(map(nlp, train['data']))\n",
    "dictionary = Dictionary(doc_lst)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary,\n",
    "                     num_topics=10, random_state=2,\n",
    "                     update_every=1, passes=10,\n",
    "                     alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>write, like, think, know, be, time, good, go, ...</td>\n",
       "      <td>From: (wheres my thing) Subject: WHAT car is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>drive, card, system, use, disk, problem, drive...</td>\n",
       "      <td>From: (Guy Kuo) Subject: SI Clock Poll - Final...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.6510</td>\n",
       "      <td>write, like, think, know, be, time, good, go, ...</td>\n",
       "      <td>From: (Thomas E Willis) Subject: PB questions....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4982</td>\n",
       "      <td>write, like, think, know, be, time, good, go, ...</td>\n",
       "      <td>From: (Joe Green) Subject: Re: Weitek P9000 ? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.4985</td>\n",
       "      <td>write, like, think, know, be, time, good, go, ...</td>\n",
       "      <td>From: (Jonathan McDowell) Subject: Re: Shuttle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.8068</td>\n",
       "      <td>write, like, think, know, be, time, good, go, ...</td>\n",
       "      <td>From: (Foxvog Douglas) Subject: Re: Rewording ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5739</td>\n",
       "      <td>x, file, X, program, use, window, image, `, ve...</td>\n",
       "      <td>From: (brian manning delaney) Subject: Brain T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.9276</td>\n",
       "      <td>drive, card, system, use, disk, problem, drive...</td>\n",
       "      <td>From: (GRUBB) Subject: Re: IDE vs SCSI Organiz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.4464</td>\n",
       "      <td>x, file, X, program, use, window, image, `, ve...</td>\n",
       "      <td>From: Subject: WIn 3.0 ICON HELP PLEASE! Organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.4253</td>\n",
       "      <td>drive, card, system, use, disk, problem, drive...</td>\n",
       "      <td>From: (Stan Kerr) Subject: Re: Sigma Designs D...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             6.0              0.9825   \n",
       "1            1             7.0              0.5140   \n",
       "2            2             6.0              0.6510   \n",
       "3            3             6.0              0.4982   \n",
       "4            4             6.0              0.4985   \n",
       "5            5             6.0              0.8068   \n",
       "6            6             1.0              0.5739   \n",
       "7            7             7.0              0.9276   \n",
       "8            8             1.0              0.4464   \n",
       "9            9             7.0              0.4253   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  write, like, think, know, be, time, good, go, ...   \n",
       "1  drive, card, system, use, disk, problem, drive...   \n",
       "2  write, like, think, know, be, time, good, go, ...   \n",
       "3  write, like, think, know, be, time, good, go, ...   \n",
       "4  write, like, think, know, be, time, good, go, ...   \n",
       "5  write, like, think, know, be, time, good, go, ...   \n",
       "6  x, file, X, program, use, window, image, `, ve...   \n",
       "7  drive, card, system, use, disk, problem, drive...   \n",
       "8  x, file, X, program, use, window, image, `, ve...   \n",
       "9  drive, card, system, use, disk, problem, drive...   \n",
       "\n",
       "                                                Text  \n",
       "0  From: (wheres my thing) Subject: WHAT car is t...  \n",
       "1  From: (Guy Kuo) Subject: SI Clock Poll - Final...  \n",
       "2  From: (Thomas E Willis) Subject: PB questions....  \n",
       "3  From: (Joe Green) Subject: Re: Weitek P9000 ? ...  \n",
       "4  From: (Jonathan McDowell) Subject: Re: Shuttle...  \n",
       "5  From: (Foxvog Douglas) Subject: Re: Rewording ...  \n",
       "6  From: (brian manning delaney) Subject: Brain T...  \n",
       "7  From: (GRUBB) Subject: Re: IDE vs SCSI Organiz...  \n",
       "8  From: Subject: WIn 3.0 ICON HELP PLEASE! Organ...  \n",
       "9  From: (Stan Kerr) Subject: Re: Sigma Designs D...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=train['data']):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=train['data'])\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
