{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getenv\n",
    "env = getenv('DATASET_DIR')\n",
    "\n",
    "NEWSGROUP_HOME = env if env is not None else '../datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models.nmf import Nmf\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import en_core_web_sm\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download 20 newsgroups dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = fetch_20newsgroups(subset='train', data_home=NEWSGROUP_HOME)\n",
    "test = fetch_20newsgroups(subset='test', data_home=NEWSGROUP_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dchhabra@stpl.ists.ca (Deepak Chhabra)\n",
      "Subject: Re: Goalie masks\n",
      "Nntp-Posting-Host: stpl.ists.ca\n",
      "Organization: Solar Terresterial Physics Laboratory, ISTS\n",
      "Lines: 21\n",
      "\n",
      "In article <120666@netnews.upenn.edu> kkeller@mail.sas.upenn.edu (Keith Keller) writes:\n",
      ">My vote goes to John Vanbiesbrouck.  His mask has a skyline of New York\n",
      ">City, and on the sides there are a bunch of bees (Beezer).  It looks\n",
      ">really sharp.\n",
      "\n",
      "Funny you should mention this; one time on HNIC Don Cherry pointed out\n",
      "Vanbiesbrouck's mask.  He _hated_ it.  I think he said something to the effect\n",
      "of:\n",
      "\"You see?  He was great last year; now he goes out and gets that dopey mask \n",
      "and he can't stop a beachball!\"\n",
      "\n",
      "You may or may not take Cherry seriously at all, but I cracked up when I heard\n",
      "it.\n",
      "\n",
      "I think Ed Belfour has the current best mask in the NHL btw.  I also like\n",
      "Moog's, and I'll give Fuhr's new one an honourable mention, although I haven't\n",
      "seen it closely yet (it looked good from a distance!).  What's also neat is\n",
      "Chevaldae's in Detroit; they call him \"Chevy\" so he has two checkered flags\n",
      "painted at the top as in an auto race.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][35], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "stop_list = [\n",
    "    \"subject\",\n",
    "    \"from\",\n",
    "    \"/\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \":\",\n",
    "    \"re\",\n",
    "    \"nntp\",\n",
    "    \"posting\",\n",
    "    \"host\",\n",
    "    \"lines\",\n",
    "    \"write\",\n",
    "    \"organization\",\n",
    "    \"keyword\",\n",
    "    \"distribution\",\n",
    "    \"news\",\n",
    "    \"software\",\n",
    "    \"university\",\n",
    "    \"like\",\n",
    "    \"think\",\n",
    "    \"+\",\n",
    "    \"$\",\n",
    "    \"s\",\n",
    "    \">\",\n",
    "    \"<\",\n",
    "    \"C\",\n",
    "    \"year\",\n",
    "    \"|\",\n",
    "    \"=\",\n",
    "    \"nt\",\n",
    "    \"o\",\n",
    "    \"article\",\n",
    "    \n",
    "]\n",
    "nlp.Defaults.stop_words.update(stop_list)\n",
    "for word in STOP_WORDS:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    lexeme.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out emails and other irrelevant stuff from the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(doc):\n",
    "    doc = [token.lemma_ for token in doc\n",
    "           if not (token.is_stop or\n",
    "                   token.is_punct or\n",
    "                   token.like_email or\n",
    "                   token.like_url or\n",
    "                   token.is_space or\n",
    "                   token.like_num or\n",
    "                   token.lemma_.lower() in stop_list)]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*train['data'][:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process texts, make a dictionary and a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# doc_lst = []\n",
    "\n",
    "for i, sent in enumerate(train['data']):\n",
    "    sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "    sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "    sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "    # sent = gensim.utils.simple_preprocess(str(sent), deacc=True)\n",
    "    # doc_lst.append(sent)\n",
    "    train['data'][i] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lst = list(nlp.pipe(train['data']))  # Limit number of entries for quicker analysis\n",
    "dictionary = Dictionary(doc_lst)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in doc_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thing', 'car', 'Maryland', 'College', 'Park', 'wonder', 'enlighten', 'car', 'see', 'day', '2-door', 'sport', 'car', 'look', 'late', '60s/', 'early', '70s', 'call', 'Bricklin', 'door', 'small', 'addition', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'engine', 'spec', 'production', 'car', 'history', 'info', 'funky', 'look', 'car', 'e', 'mail', 'Thanks', 'IL', 'bring', 'neighborhood', 'Lerxst']\n",
      "['Guy', 'Kuo', 'SI', 'Clock', 'Poll', 'Final', 'Summary', 'Final', 'SI', 'clock', 'report', 'Keywords', 'SI', 'acceleration', 'clock', 'upgrade', 'I.D.', 'shelley.1qvfo9INNc3s', 'Washington', 'fair', 'numb', 'brave', 'soul', 'upgrade', 'SI', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', 'speed', 'attain', 'CPU', 'rate', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', 'be', 'floppy', 'especially', 'request', 'summarize', 'day', 'add', 'network', 'knowledge', 'base', 'clock', 'upgrade', 'answer', 'poll', 'Thanks', 'Guy', 'Kuo']\n",
      "['Thomas', 'E', 'Willis', 'PB', 'question', 'Purdue', 'Engineering', 'Computer', 'Network', 'usa', 'folk', 'mac', 'plus', 'finally', 'give', 'ghost', 'weekend', 'start', 'life', '512k', 'way', 'sooo', 'be', 'market', 'new', 'machine', 'bite', 'soon', 'intend', 'be', 'look', 'pick', 'powerbook', 'maybe', 'bunch', 'question', 'hopefully', 'somebody', 'answer', 'anybody', 'know', 'dirt', 'round', 'powerbook', 'introduction', 'expect', 'have', 'hear', '185c', 'suppose', 'appearence', 'summer', 'hear', 'anymore', 'access', 'macleak', 'wonder', 'anybody', 'info', 'anybody', 'hear', 'rumor', 'price', 'drop', 'powerbook', 'line', 'one', 'duo', 'go', 'recently', 'impression', 'display', 'probably', 'swing', 'get', '80Mb', 'disk', 'feel', 'well', 'display', 'yea', 'look', 'great', 'store', 'wow', 'good', 'solicit', 'opinion', 'people', 'use', 'day', 'day', 'worth', 'take', 'disk', 'size', 'money', 'hit', 'active', 'display', 'realize', 'real', 'subjective', 'question', 'have', 'play', 'machine', 'computer', 'store', 'breifly', 'figure', 'opinion', 'somebody', 'actually', 'use', 'machine', 'daily', 'prove', 'helpful', 'hellcats', 'perform', 'thank', 'bunch', 'advance', 'info', 'email', 'ill', 'post', 'summary', 'read', 'time', 'premium', 'final', 'corner', 'Tom', 'Willis', 'Purdue', 'Electrical', 'Engineering', 'Convictions', 'dangerous', 'enemy', 'truth', 'lie', 'F.', 'W.', 'Nietzsche']\n"
     ]
    }
   ],
   "source": [
    "print(*doc_lst[:3], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit NMF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus=corpus, id2word=dictionary,\n",
    "          num_topics=20, normalize=True,\n",
    "          random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"File\" + 0.006*\"program\" + 0.006*\"file\" + 0.005*\"drive\" + 0.005*\"work\" + 0.005*\"use\" + 0.004*\"image\" + 0.004*\"have\" + 0.004*\"time\" + 0.003*\"launch\"'),\n",
       " (1,\n",
       "  '0.015*\"Armenian\" + 0.014*\"Turkish\" + 0.009*\"Jew\" + 0.009*\"people\" + 0.005*\"Turkey\" + 0.004*\"say\" + 0.004*\"Turk\" + 0.004*\"book\" + 0.003*\"come\" + 0.003*\"Soviet\"'),\n",
       " (2,\n",
       "  '0.011*\"say\" + 0.008*\"know\" + 0.007*\"go\" + 0.006*\"people\" + 0.006*\"come\" + 0.006*\"available\" + 0.005*\"tell\" + 0.004*\"time\" + 0.004*\"include\" + 0.004*\"Armenian\"'),\n",
       " (3,\n",
       "  '0.011*\"`\" + 0.008*\"DOS\" + 0.007*\"Armenian\" + 0.006*\"know\" + 0.006*\"say\" + 0.006*\"people\" + 0.006*\"go\" + 0.003*\"come\" + 0.003*\"want\" + 0.003*\"time\"'),\n",
       " (4,\n",
       "  '0.832*\"AX\" + 0.059*\"MAX\" + 0.007*\"G)R\" + 0.004*\"G9V\" + 0.002*\"GIZ\" + 0.001*\"M\" + 0.001*\"G\" + 0.001*\"T\" + 0.001*\"MR\" + 0.000*\"`\"'),\n",
       " (5,\n",
       "  '0.033*\"`\" + 0.011*\"anonymous\" + 0.009*\"privacy\" + 0.009*\"internet\" + 0.009*\"use\" + 0.008*\"email\" + 0.007*\"information\" + 0.007*\"user\" + 0.007*\"file\" + 0.007*\"system\"'),\n",
       " (6,\n",
       "  '0.014*\"File\" + 0.004*\"gun\" + 0.003*\"launch\" + 0.003*\"States\" + 0.003*\"United\" + 0.003*\"file\" + 0.003*\"Gun\" + 0.003*\"firearm\" + 0.003*\"Bill\" + 0.003*\"Mr.\"'),\n",
       " (7,\n",
       "  '0.026*\"W\" + 0.006*\"Jesus\" + 0.006*\"say\" + 0.006*\"people\" + 0.005*\"M\" + 0.004*\"know\" + 0.004*\"UW\" + 0.004*\"come\" + 0.004*\"time\" + 0.003*\"day\"'),\n",
       " (8,\n",
       "  '0.010*\"available\" + 0.007*\"include\" + 0.006*\"server\" + 0.006*\"image\" + 0.006*\"version\" + 0.006*\"widget\" + 0.006*\"pub\" + 0.005*\"file\" + 0.005*\"base\" + 0.005*\"mail\"'),\n",
       " (9,\n",
       "  '0.822*\"AX\" + 0.057*\"MAX\" + 0.023*\"G9V\" + 0.008*\"G)R\" + 0.003*\"GIZ\" + 0.002*\"MG9V\" + 0.002*\"G9V=\" + 0.001*\"G\" + 0.001*\"MR\" + 0.001*\"M\"'),\n",
       " (10,\n",
       "  '0.188*\"X\" + 0.014*\"file\" + 0.010*\"entry\" + 0.009*\"oname\" + 0.009*\"output\" + 0.008*\"program\" + 0.008*\"char\" + 0.006*\"stream\" + 0.005*\"rule\" + 0.005*\"EOF_NOT_OK\"'),\n",
       " (11,\n",
       "  '0.009*\"W\" + 0.007*\"people\" + 0.005*\"know\" + 0.005*\"`\" + 0.004*\"anonymous\" + 0.004*\"say\" + 0.003*\"come\" + 0.003*\"want\" + 0.003*\"system\" + 0.003*\"right\"'),\n",
       " (12,\n",
       "  '0.142*\"X\" + 0.012*\"file\" + 0.008*\"oname\" + 0.007*\"output\" + 0.007*\"char\" + 0.006*\"`\" + 0.006*\"entry\" + 0.005*\"program\" + 0.005*\"stream\" + 0.004*\"tobacco\"'),\n",
       " (13,\n",
       "  '0.008*\"Jesus\" + 0.005*\"available\" + 0.005*\"use\" + 0.004*\"include\" + 0.004*\"widget\" + 0.004*\"Matthew\" + 0.003*\"tobacco\" + 0.003*\"prophecy\" + 0.003*\"`\" + 0.003*\"Motif\"'),\n",
       " (14,\n",
       "  '0.144*\"X\" + 0.010*\"file\" + 0.009*\"W\" + 0.008*\"oname\" + 0.007*\"output\" + 0.007*\"char\" + 0.006*\"program\" + 0.006*\"entry\" + 0.005*\"stream\" + 0.004*\"||\"'),\n",
       " (15,\n",
       "  '0.009*\"people\" + 0.007*\"say\" + 0.007*\"go\" + 0.007*\"know\" + 0.005*\"have\" + 0.005*\"time\" + 0.005*\"come\" + 0.005*\"work\" + 0.004*\"Q\" + 0.004*\"want\"'),\n",
       " (16,\n",
       "  '0.016*\"W\" + 0.007*\"say\" + 0.006*\"Jesus\" + 0.005*\"people\" + 0.005*\"Armenian\" + 0.005*\"argument\" + 0.005*\"know\" + 0.004*\"time\" + 0.004*\"M\" + 0.004*\"come\"'),\n",
       " (17,\n",
       "  '0.043*\"W\" + 0.014*\"M\" + 0.006*\"UW\" + 0.006*\"T\" + 0.004*\"^\" + 0.004*\"W1\" + 0.003*\"MW\" + 0.003*\"WW\" + 0.002*\"tobacco\" + 0.002*\"Health\"'),\n",
       " (18,\n",
       "  '0.036*\"W\" + 0.012*\"M\" + 0.005*\"UW\" + 0.004*\"T\" + 0.003*\"use\" + 0.003*\"W1\" + 0.003*\"^\" + 0.003*\"system\" + 0.002*\"tape\" + 0.002*\"drive\"'),\n",
       " (19,\n",
       "  '0.161*\"X\" + 0.017*\"file\" + 0.010*\"program\" + 0.008*\"oname\" + 0.008*\"output\" + 0.007*\"char\" + 0.007*\"entry\" + 0.006*\"include\" + 0.005*\"build\" + 0.005*\"line\"')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%pixie_debugger` not found.\n"
     ]
    }
   ],
   "source": [
    "%%pixie_debugger\n",
    "\n",
    "# Something here constantly kills the kernelâ€¦\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_list,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = nmf.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=40)\n",
    "    plt.gca().imshow(cloud, interpolation='bilinear')\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence metric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5314442072480839"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_nmf = CoherenceModel(model=nmf, texts=doc_lst, dictionary=dictionary, coherence='c_v')\n",
    "coherence = coherence_nmf.get_coherence()\n",
    "coherence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
